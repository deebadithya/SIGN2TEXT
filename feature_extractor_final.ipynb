{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "path = os.path.abspath('')\n",
    "vid_src_path = os.path.abspath(\"DataSet//Videos//ori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# contains the main dataset.\n",
    "with open(\"MSASL_train.json\", 'r') as file:\n",
    "    j_data = json.load(file)\n",
    "\n",
    "# contains the each label's(key) indexes(index of each module in main dataset) as list(value).\n",
    "with open(\"labelsNindex.json\",'r') as file:\n",
    "    labelsNindex = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains the video's name with its human square crop cordinates compatible with cv2 as frame[y1, y2:x1, x2].\n",
    "with open(\"videoNcrop.json\",'r') as file:\n",
    "    videoNcrop = json.load(file)\n",
    "    \n",
    "# contains the label's count(value) of each label_name(key).\n",
    "with open(\"labelNcounts.json\",'r') as file:\n",
    "    labelsNcount = json.load(file)\n",
    "\n",
    "# contains the top 10 most counts contained labels and its count(no. of modules).\n",
    "with open(\"top10labelNcount.json\",'r') as file:\n",
    "    top10labelNcount = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detects and returnsthe landmarks of humans as results.\n",
    "def mediapipe_detection(image,model):\n",
    "    # cv.flip(image,1)\n",
    "    image = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draws the landmarks of face, pose, hands.\n",
    "def draw_landmarks(image, results):\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp.solutions.face_mesh_connections.FACEMESH_FACE_OVAL,\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,76), thickness=2, circle_radius=4),#for points\n",
    "                              mp_drawing.DrawingSpec(color=(80,256,250), thickness=2, circle_radius=2))#for connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets keypoints of result's landmaks and returns as single flatten landmarks np.array().\n",
    "def keypoints_extraction(results):\n",
    "    face =  np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    pose = np.array([[res.x, res.y,res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3) # removed res.visibility()\n",
    "    lh = np.array([[res.x, res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check's existence of paticular file without need of its extension(ex: .mp4 or webm).\n",
    "def check_existence(file_path, file_name):\n",
    "    \n",
    "    for file in os.listdir(file_path):\n",
    "        name = str(file).split('.')\n",
    "        ext = name[-1]\n",
    "        if len(name) == 2:\n",
    "            name = \"\".join(name[:-1])\n",
    "        else:\n",
    "            name = \".\".join(name[:-1])\n",
    "        if (name == file_name) and (os.path.isfile(os.path.join(file_path,file_name + \".\" + ext))):\n",
    "            return file_name + \".\" + ext\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts normalised cordinates into pixel cordinates\n",
    "def locate_it(norm_val, w, h):\n",
    "    try:\n",
    "        point = (int(norm_val.x * w),int(norm_val.y * h))\n",
    "    except AttributeError as ae:\n",
    "        point = (int(norm_val[0] * w),int(norm_val[1] * h))\n",
    "\n",
    "    point = (min(point[0], w - 1),min(point[1], h - 1))\n",
    "\n",
    "    return point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rect_cords(frame,results):\n",
    "    \n",
    "    height, width, channels = frame.shape\n",
    "    top_head_index_in_face_landmarks = 10\n",
    "    hip_index_in_pose_landmarks = (24,23)\n",
    "\n",
    "    left_shoulder = results.pose_landmarks.landmark[11]\n",
    "    right_shoulder = results.pose_landmarks.landmark[12]\n",
    "\n",
    "    centre_point = ((left_shoulder.x + right_shoulder.x) / 2 , (left_shoulder.y + right_shoulder.y) / 2)\n",
    "    \n",
    "    if not results.face_landmarks:\n",
    "        face_loc = results.face_landmarks.landmark[top_head_index_in_face_landmarks]\n",
    "        st_norm_val = (face_loc.x,face_loc.y)\n",
    "        head_point = locate_it(face_loc,width,height)\n",
    "    \n",
    "    else:\n",
    "        left_eyebrow_inner = results.pose_landmarks.landmark[4]\n",
    "        # right_eyebrow_inner = results.pose_landmarks.landmark[1]\n",
    "        nose_point = results.pose_landmarks.landmark[0]\n",
    "\n",
    "        st_norm_val = [nose_point.x,nose_point.y]\n",
    "        norm_dist = cv.norm(np.array([left_eyebrow_inner.x,left_eyebrow_inner.y]), np.array(st_norm_val))\n",
    "        st_norm_val = (st_norm_val[0], st_norm_val[1] - norm_dist*3)\n",
    "\n",
    "        nose_point = locate_it(nose_point, width, height)\n",
    "        left_eyebrow_inner = locate_it(left_eyebrow_inner, width, height)\n",
    "\n",
    "        # face_loc = ((left_eyebrow_inner.x + right_eyebrow_inner.x) / 2 ,  nose_point.y - (left_eyebrow_inner.y + right_eyebrow_inner.y) / 2) \n",
    "        dist  = cv.norm(np.array(left_eyebrow_inner), np.array(nose_point))\n",
    "        head_point = (nose_point[0], nose_point[1] - int(dist)*3)\n",
    "        \n",
    "    pose_loc_1 = results.pose_landmarks.landmark[hip_index_in_pose_landmarks[0]]\n",
    "    pose_loc_2 = results.pose_landmarks.landmark[hip_index_in_pose_landmarks[1]]\n",
    "\n",
    "        \n",
    "    pose_point_1 = locate_it(pose_loc_1,width,height)\n",
    "    pose_point_2 = locate_it(pose_loc_2,width,height)\n",
    "    pose_point = (int((pose_point_1[0]+pose_point_2[0])/2),int((pose_point_1[1]+pose_point_2[1])/2))\n",
    "    distance = int(math.sqrt((pose_point[0] - head_point[0]) ** 2 + (pose_point[1] - head_point[1]) ** 2))\n",
    "\n",
    "\n",
    "    # adjusting the resolution with reducing crop size when there is less space.\n",
    "    if head_point[0]-int(distance/2)-29 > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)-30\n",
    "        st_pt2 = pose_point[0]+int(distance/2)+30\n",
    "\n",
    "    elif head_point[0]-int(distance/2)-19 > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)-20\n",
    "        st_pt2 = pose_point[0]+int(distance/2)+20\n",
    "\n",
    "\n",
    "    elif head_point[0]-int(distance/2)-9 > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)-10\n",
    "        st_pt2 = pose_point[0]+int(distance/2)+10\n",
    "\n",
    "    else: #head_point[0]-int(distance/2) > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)\n",
    "        st_pt2 = pose_point[0]+int(distance/2)\n",
    "\n",
    "    head_point = (max(st_pt1,0),head_point[1]-10 if head_point[1]-10 > 0 else 0)\n",
    "    pose_point = (min(st_pt2, width - 1), min(pose_point[1], height - 1))\n",
    "\n",
    "    rects_st_pt = head_point\n",
    "    rects_sp_pt = pose_point\n",
    "    \n",
    "    return [rects_st_pt, rects_sp_pt],centre_point,st_norm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features  = np.array(['eat','fish','milk', 'cousin', 'want', 'nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the average video duration(frame count) of the all features\n",
    "feature_indNdura = []\n",
    "for feature in features:\n",
    "\n",
    "    indexNduration = {}\n",
    "    indexes_of_feature = labelsNindex[feature]\n",
    "\n",
    "    for index in indexes_of_feature:\n",
    "        \n",
    "        # obtaining specific data from j_data and storing it on variable.\n",
    "        speci_data = j_data[index]\n",
    "\n",
    "        file_name = speci_data['file']\n",
    "\n",
    "        # now we have to check whether the obtained index contains valid video data or not.\n",
    "        \n",
    "        is_video_there = check_existence(vid_src_path, file_name)\n",
    "        if is_video_there:\n",
    "            st_frame,end_frame = speci_data['start'], speci_data['end']\n",
    "            vid_frame_duration = end_frame - st_frame\n",
    "            indexNduration[index] = vid_frame_duration\n",
    "    feature_indNdura.append(indexNduration)\n",
    "# extracting the minimum, maximum and average duration(frame count) of the fetures sign sources.\n",
    "min_fr_len = 1000\n",
    "max_fr_len = 0\n",
    "sum_for_calc_avg = []\n",
    "for feature in range(len(features)):\n",
    "    for duration in feature_indNdura[feature].values():\n",
    "        if duration > max_fr_len:\n",
    "            max_fr_len = duration\n",
    "        if duration < min_fr_len:\n",
    "            min_fr_len = duration\n",
    "        sum_for_calc_avg.append(duration)\n",
    "avg_fr_len = int(sum(sum_for_calc_avg)/len(sum_for_calc_avg))\n",
    "\n",
    "print(\"the minimum length of the sign is {}\".format(min_fr_len))\n",
    "print(\"the maximum length of the sign is {}\".format(max_fr_len))\n",
    "print(\"The average length of the sign is {}\".format(avg_fr_len))  \n",
    "# obataining the average video sources of the features.\n",
    "no_of_src_vids = 0\n",
    "for feature in range(len(features)):\n",
    "    no_vids = len(feature_indNdura[feature])\n",
    "    no_of_src_vids += no_vids\n",
    "avg_vid_count = no_of_src_vids // len(features)\n",
    "print(avg_vid_count)\n",
    "\n",
    "# exporting calculated values.\n",
    "var_store = {'avg_vid_count':avg_vid_count, 'min_fr_len':min_fr_len , 'avg_fr_len': avg_fr_len, 'max_fr_len':max_fr_len , 'features': list(features)}\n",
    "with open(\"var_store.json\", 'w') as file:\n",
    "    json.dump(var_store, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the number of folders for the features as per the average.\n",
    "for feature in range(len(features)):\n",
    "    os.makedirs(os.path.join(path,\"DataSet\",\"Keypoints\",str(feature)), exist_ok = True)\n",
    "    for index in range(avg_vid_count):\n",
    "        os.makedirs(os.path.join(path,\"DataSet\",\"Keypoints\",str(feature),str(index)), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frame_duration = 1000\n",
    "max_frame_duration = 0\n",
    "total_frame_durations = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_frame_duration = 35\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic: \n",
    "    # features indicates each label's index ex: features[0] = 'eat'\n",
    "    # looping through each feature with its index.\n",
    "    for feature in range(len(features)):\n",
    "        # extracting the features index's from the labelNindex file and storing it onto a variable.\n",
    "        features_index = []\n",
    "        # now the features_index holds the indexs of that specific features from the MSASL.json file.\n",
    "        vid_indexes_as_list = list(feature_indNdura[feature].keys())\n",
    "\n",
    "        add_on_count = 0\n",
    "        \n",
    "        # sequence represents a single video and range's about average video count from each feature among features.\n",
    "        for sequence in range(avg_vid_count):\n",
    "            # managing to assingn when there is lesser vid src contents than average then chose one from exist src's.\n",
    "            try:\n",
    "                src_index = vid_indexes_as_list[sequence]\n",
    "            except IndexError as ie:\n",
    "                print(\"list out of range... Generating random index from existing...\")\n",
    "                src_index = random.choice(vid_indexes_as_list)\n",
    "\n",
    "            # when faced invalid video's then generating random video.\n",
    "            while(True):\n",
    "                file_name_ext =  check_existence(vid_src_path, j_data[src_index]['file'])\n",
    "                if not file_name_ext:\n",
    "                    src_index = random.choice(vid_indexes_as_list)\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            # getting width, height from spec_data.\n",
    "            spec_data = j_data[src_index]\n",
    "            width,height = spec_data['width'], spec_data['height']\n",
    "\n",
    "            # getting start and end frame number from spec_data\n",
    "            st_frame,sp_frame = spec_data['start'], spec_data['end']\n",
    "      \n",
    "            print(\"Working with the file : {} from {}:{}\".format(file_name_ext,st_frame,sp_frame))\n",
    "\n",
    "            # sequence_path = os.path.join(feature_path, str(sequence)+\".mp4\")\n",
    "            cap = cv.VideoCapture(os.path.join(vid_src_path,file_name_ext))\n",
    "\n",
    "            duration = sp_frame - st_frame\n",
    "            \n",
    "            # setting the starting frame number to the cap.\n",
    "            cap.set(cv.CAP_PROP_POS_FRAMES, st_frame)\n",
    "            \n",
    "            # initially considering the inside video frame reading can continue and the current_frame with st_Frame.\n",
    "            can_continue = True\n",
    "            current_frame = st_frame\n",
    "            frame_number = 0\n",
    "            idle_gest_reg = np.array([])\n",
    "            mode = \"Loading...\"\n",
    "            can_record = False\n",
    "            frame_track = False\n",
    "            last_crop = [(0,0), (1, 1)]\n",
    "                \n",
    "            # looping through average frame_number of videos of label's sign duration.\n",
    "            \n",
    "            #creating a total of 35 frames only sign duration for signs.\n",
    "            temp_key_storage = np.array([],dtype=np.uint8).reshape(0,1086)\n",
    "            \n",
    "            for fr in range(duration):\n",
    "\n",
    "                # check whether to continue reading video frames or not.\n",
    "                if can_continue:\n",
    "\n",
    "                    ret, ori_img = cap.read()\n",
    "                    if ret:\n",
    "                        crop_frame = copy.deepcopy(ori_img)\n",
    "                        img_result , results = mediapipe_detection(ori_img, holistic)\n",
    "                        keypoints = keypoints_extraction(results)\n",
    "                        try:\n",
    "                            crop, c_point, st_norm_pt = get_rect_cords(ori_img, results)\n",
    "\n",
    "                            if cv.norm(crop[0], last_crop[0]) > 4 and cv.norm(crop[1], last_crop[1]) > 4:\n",
    "                                pass                  \n",
    "                            else:\n",
    "                                crop = last_crop\n",
    "\n",
    "                            last_crop = crop\n",
    "                        except AttributeError as ae:\n",
    "                            crop = last_crop\n",
    "\n",
    "                        crop_frame = crop_frame[crop[0][1]:crop[1][1], crop[0][0]:crop[1][0]]                 \n",
    "                        standard_frame = copy.deepcopy(crop_frame)\n",
    "\n",
    "                        s_height = 300\n",
    "                        s_width = 300\n",
    "\n",
    "                        width_scale = s_height/crop_frame.shape[1] \n",
    "                        height_scale = s_width/crop_frame.shape[0] \n",
    "\n",
    "                        standard_frame = cv.resize(standard_frame, (s_height, s_width))\n",
    "                        custom_keypoints = np.array([])\n",
    "                                                                \n",
    "\n",
    "                        st_pt = (crop[0][0]/ori_img.shape[1], crop[0][1]/ori_img.shape[0])\n",
    "\n",
    "                        for i in range(0, len(keypoints), 3):\n",
    "                            \n",
    "                            loc_ = (keypoints[i] - st_pt[0], keypoints[i+1] - st_pt[1])\n",
    "                            point = locate_it(loc_,ori_img.shape[1],ori_img.shape[0])\n",
    "                            standard_point = (int(point[0] * width_scale), int(point[1] * height_scale))\n",
    "\n",
    "                            norm_standard_point = [standard_point[0] / s_width, standard_point[1] / s_height]\n",
    "\n",
    "                            if (0 > standard_point[0] > s_width or 0 > standard_point[1] > s_height):\n",
    "                                print(\"this point went into invalid section:\")\n",
    "                                print(\"standard_point: {}\".format(standard_point))\n",
    "                                norm_stand_point = [0.0, 0.0]\n",
    "                        \n",
    "                            cv.circle(standard_frame, standard_point, 1, (255,255, 0), 1)\n",
    "                            custom_keypoints = np.append(custom_keypoints, norm_standard_point)\n",
    "                        \n",
    "                        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "                            idle_gest_reg = np.append(idle_gest_reg, 1)\n",
    "                            can_record = True\n",
    "                            frame_track = True\n",
    "                        else:\n",
    "                            idle_gest_reg = np.append(idle_gest_reg, None)\n",
    "\n",
    "                        if can_record:\n",
    "                            if len(idle_gest_reg) > 7:\n",
    "                                idle_gest_reg = idle_gest_reg[-7:]\n",
    "                                 \n",
    "                            if len(idle_gest_reg) == 7:\n",
    "                                if np.all(idle_gest_reg == None):        \n",
    "                                    mode = \"idle\"\n",
    "                                    print(\"Gesture Ended! Stopping...\")\n",
    "                                    can_record = False\n",
    "                                    frame_track = False\n",
    "                                    print(frame_number)\n",
    "                                    frame_number = 0   \n",
    "                                    break\n",
    "                                else:\n",
    "                                    mode = \"gest\"\n",
    "                                    frame_number += 1\n",
    "                                    \n",
    "                        cv.putText(standard_frame, \"{}{}:{}:FN:{}\".format(features[feature],sequence,mode, frame_number), (2, 20), cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 0, cv.LINE_AA)     \n",
    "                        cv.imshow('standard_Frame', standard_frame)\n",
    "                        \n",
    "\n",
    "                        if current_frame >= sp_frame:\n",
    "                            can_continue = False\n",
    "                            break\n",
    "                                               \n",
    "                        if (cv.waitKey(1) & 0xFF == ord('q')):\n",
    "                            break\n",
    "                           \n",
    "                    else:\n",
    "                        can_continue = False\n",
    "                        break\n",
    "\n",
    "                           \n",
    "                else:\n",
    "                    print(\"last frame reached still reading zero array frame...\")\n",
    "                    break\n",
    "   \n",
    "\n",
    "            \n",
    "                current_frame += 1  \n",
    "                \n",
    "                if frame_track:\n",
    "                    if frame_number >= custom_frame_duration:\n",
    "                        break\n",
    "                \n",
    "\n",
    "                # saving the numpy array as .npy file.\n",
    "                if can_record:\n",
    "                    \n",
    "                    temp_key_storage = np.vstack((temp_key_storage,custom_keypoints))\n",
    "           \n",
    "            \n",
    "            if mode == \"Loading...\":\n",
    "                add_on_count -= 1\n",
    "                print(\"skipping this video..Had no gesture but adding randomly chosened prev video contenets.\")\n",
    "                \n",
    "                # filling the remaining invalid gap with previous video extracted data's.\n",
    "                temp_random = random.randint(0, sequence - 1)\n",
    "                for frame_num in range(custom_frame_duration):\n",
    "                        prev_npy_path = os.path.join(path,\"DataSet\",\"Keypoints\",str(feature),str(temp_random),str(frame_num))\n",
    "                        np_data = np.load(npy_path)\n",
    "                        npy_path = os.path.join(path,\"DataSet\",\"Keypoints\",str(feature),str(sequence),str(frame_num))\n",
    "                        np.save(np_data, npy_path)\n",
    "            else:\n",
    "                key_length = len(temp_key_storage)\n",
    "                if  key_length > custom_frame_duration:\n",
    "                    print(\"exited frame lim about {}.. so reducing to standard size 35.\".format(key_length))\n",
    "                    center_pt = int(key_length/2)\n",
    "                    starting_pt = center_pt - int(custom_frame_duration/2)\n",
    "                    temp_key_storage = temp_key_storage[starting_pt:starting_pt+35]\n",
    "                else:\n",
    "                    refill_len = custom_frame_duration - key_length\n",
    "                    print(\"filling the less frame gap of {} frames\".format(refill_len))\n",
    "                    for i in range(refill_len):\n",
    "                        temp_key_storage = np.vstack((temp_key_storage,np.zeros(1086, dtype=np.float64)))\n",
    "                \n",
    "                if len(temp_key_storage) == custom_frame_duration:\n",
    "                    for frame_num in range(custom_frame_duration):\n",
    "                        npy_path = os.path.join(path,\"DataSet\",\"Keypoints\",str(feature),str(sequence),str(frame_num))\n",
    "                        np.save(npy_path, custom_keypoints) \n",
    "                else:\n",
    "                    raise ValueError(\"invalid array size...\")\n",
    "                \n",
    "                total_frame_durations = np.append(total_frame_durations, frame_number)\n",
    "            print(\"Completed the sequence NO: {}\".format(sequence))\n",
    "            cap.release()\n",
    "        print(\"completed the feature NO: {}\".format(feature))\n",
    "print(\"got completed everything...\")  \n",
    "\n",
    "avg_frame_duration = int(np.mean(total_frame_durations))\n",
    "\n",
    "# with open(\"videoNcrop.json\", 'w') as file:\n",
    "#     json.dump(videoNcrop, file)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
