{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Conv1D,MaxPooling1D, Flatten\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"eat\", \"fish\", \"milk\", \"cousin\", \"want\", \"nice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# CNN layers\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(35, 1086)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "# LSTM layers\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(features.shape[0], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model_data_17(idle_gest_iso_reduMax2).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detects and returnsthe landmarks of humans as results.\n",
    "def mediapipe_detection(image,model):\n",
    "    # cv.flip(image,1)\n",
    "    image = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets keypoints of result's landmaks and returns as single flatten landmarks np.array().\n",
    "def keypoints_extraction(results):\n",
    "    face =  np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    pose = np.array([[res.x, res.y,res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3) # removed res.visibility()\n",
    "    lh = np.array([[res.x, res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts normalised cordinates into pixel cordinates\n",
    "def locate_it(norm_val, w, h):\n",
    "    try:\n",
    "        point = (int(norm_val.x * w),int(norm_val.y * h))\n",
    "    except AttributeError as ae:\n",
    "        point = (int(norm_val[0] * w),int(norm_val[1] * h))\n",
    "\n",
    "    point = (min(point[0], w - 1),min(point[1], h - 1))\n",
    "\n",
    "    return point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rect_cords(frame,results):\n",
    "    \n",
    "    try:\n",
    "        height, width, channels = frame.shape\n",
    "    except AttributeError as ae:\n",
    "        height, width = frame.size\n",
    "    top_head_index_in_face_landmarks = 10\n",
    "    hip_index_in_pose_landmarks = (24,23)\n",
    "\n",
    "    left_shoulder = results.pose_landmarks.landmark[11]\n",
    "    right_shoulder = results.pose_landmarks.landmark[12]\n",
    "\n",
    "    centre_point = ((left_shoulder.x + right_shoulder.x) / 2 , (left_shoulder.y + right_shoulder.y) / 2)\n",
    "    \n",
    "    if not results.face_landmarks:\n",
    "        face_loc = results.face_landmarks.landmark[top_head_index_in_face_landmarks]\n",
    "        st_norm_val = (face_loc.x,face_loc.y)\n",
    "        head_point = locate_it(face_loc,width,height)\n",
    "    \n",
    "    else:\n",
    "        left_eyebrow_inner = results.pose_landmarks.landmark[4]\n",
    "        # right_eyebrow_inner = results.pose_landmarks.landmark[1]\n",
    "        nose_point = results.pose_landmarks.landmark[0]\n",
    "\n",
    "        st_norm_val = [nose_point.x,nose_point.y]\n",
    "        norm_dist = cv.norm(np.array([left_eyebrow_inner.x,left_eyebrow_inner.y]), np.array(st_norm_val))\n",
    "        st_norm_val = (st_norm_val[0], st_norm_val[1] - norm_dist*3)\n",
    "\n",
    "        nose_point = locate_it(nose_point, width, height)\n",
    "        left_eyebrow_inner = locate_it(left_eyebrow_inner, width, height)\n",
    "\n",
    "        # face_loc = ((left_eyebrow_inner.x + right_eyebrow_inner.x) / 2 ,  nose_point.y - (left_eyebrow_inner.y + right_eyebrow_inner.y) / 2) \n",
    "        dist  = cv.norm(np.array(left_eyebrow_inner), np.array(nose_point))\n",
    "        head_point = (nose_point[0], nose_point[1] - int(dist)*3)\n",
    "        \n",
    "    pose_loc_1 = results.pose_landmarks.landmark[hip_index_in_pose_landmarks[0]]\n",
    "    pose_loc_2 = results.pose_landmarks.landmark[hip_index_in_pose_landmarks[1]]\n",
    "\n",
    "        \n",
    "    pose_point_1 = locate_it(pose_loc_1,width,height)\n",
    "    pose_point_2 = locate_it(pose_loc_2,width,height)\n",
    "    pose_point = (int((pose_point_1[0]+pose_point_2[0])/2),int((pose_point_1[1]+pose_point_2[1])/2))\n",
    "    distance = int(math.sqrt((pose_point[0] - head_point[0]) ** 2 + (pose_point[1] - head_point[1]) ** 2))\n",
    "\n",
    "\n",
    "    # adjusting the resolution with reducing crop size when there is less space.\n",
    "    if head_point[0]-int(distance/2)-29 > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)-30\n",
    "        st_pt2 = pose_point[0]+int(distance/2)+30\n",
    "\n",
    "    elif head_point[0]-int(distance/2)-19 > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)-20\n",
    "        st_pt2 = pose_point[0]+int(distance/2)+20\n",
    "\n",
    "\n",
    "    elif head_point[0]-int(distance/2)-9 > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)-10\n",
    "        st_pt2 = pose_point[0]+int(distance/2)+10\n",
    "\n",
    "    else: #head_point[0]-int(distance/2) > 0:\n",
    "        st_pt1 = head_point[0]-int(distance/2)\n",
    "        st_pt2 = pose_point[0]+int(distance/2)\n",
    "\n",
    "\n",
    "    head_point = (max(st_pt1,0),head_point[1]-10 if head_point[1]-10 > 0 else 0)\n",
    "    pose_point = (min(st_pt2, width - 1), min(pose_point[1], height - 1))\n",
    "\n",
    "    rects_st_pt = head_point\n",
    "    rects_sp_pt = pose_point\n",
    "    \n",
    "    return [rects_st_pt, rects_sp_pt],centre_point,st_norm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "sequence = []\n",
    "threshold = 0.30\n",
    "predictions = []\n",
    "current_frame = 0\n",
    "can_record = False\n",
    "mode = \"Loading...\"\n",
    "idle_gest_reg = np.array([])\n",
    "\n",
    "can_do_live_text = False\n",
    "live_text_frame_number = 0\n",
    "last_crop = [(0,0),(0+300, 0+300)]\n",
    "\n",
    "# videoNcrop is used to cropping resolution as pre-defined.\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # initially considering the inside video frame reading can continue and the current_frame with st_Frame.\n",
    "    can_continue = True\n",
    "    frame_number = 1\n",
    "\n",
    "    while can_continue:\n",
    "\n",
    "        ret, ori_img = cap.read()\n",
    "\n",
    "        if ret:\n",
    "            crop_frame = copy.deepcopy(ori_img)\n",
    "            # print(ori_img.shape[1],\" X \",ori_img.shape[0])\n",
    "            img_result , results = mediapipe_detection(ori_img, holistic)\n",
    "            keypoints = keypoints_extraction(results)\n",
    "            try:\n",
    "                crop, c_point, st_norm_pt = get_rect_cords(ori_img, results)\n",
    "                last_crop = crop\n",
    "            except AttributeError as ae:\n",
    "                crop = last_crop\n",
    "\n",
    "            crop_frame = crop_frame[crop[0][1]:crop[1][1], crop[0][0]:crop[1][0]]\n",
    "        \n",
    "            standard_frame = copy.deepcopy(crop_frame)\n",
    "\n",
    "            s_height = 300\n",
    "            s_width = 300\n",
    "\n",
    "            width_scale = s_height/crop_frame.shape[1] \n",
    "            height_scale = s_width/crop_frame.shape[0] \n",
    "\n",
    "            standard_frame = cv.resize(standard_frame, (s_height, s_width))\n",
    "            custom_keypoints = np.array([])\n",
    "        \n",
    "            if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "                idle_gest_reg = np.append(idle_gest_reg, 1)\n",
    "                can_record = True\n",
    "                frame_track = True\n",
    "            else:\n",
    "                idle_gest_reg = np.append(idle_gest_reg, None)\n",
    "\n",
    "            if can_record:\n",
    "                if len(idle_gest_reg) > 7:\n",
    "                    idle_gest_reg = idle_gest_reg[-7:]\n",
    "                        \n",
    "                if len(idle_gest_reg) == 7:\n",
    "                    if np.all(idle_gest_reg == None):        \n",
    "                        mode = \"idle\"\n",
    "                        print(\"Gesture Ended! Stopping...\")\n",
    "                        can_record = False\n",
    "                        frame_track = False\n",
    "                        frame_number = 1\n",
    "                        sequence = []\n",
    "                        \n",
    "                    else:\n",
    "                        mode = \"gest\"\n",
    "                        frame_number +=1\n",
    "            \n",
    "            \n",
    "            st_pt = (crop[0][0]/ori_img.shape[1], crop[0][1]/ori_img.shape[0])\n",
    "\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                \n",
    "                loc_ = (keypoints[i] - st_pt[0], keypoints[i+1] - st_pt[1])\n",
    "                point = locate_it(loc_,ori_img.shape[1],ori_img.shape[0])\n",
    "                standard_point = (int(point[0] * width_scale), int(point[1] * height_scale))\n",
    "\n",
    "                norm_standard_point = [standard_point[0] / s_width, standard_point[1] / s_height]\n",
    "\n",
    "                if (0 > standard_point[0] > s_width or 0 > standard_point[1] > s_height):\n",
    "                    print(\"this point went into invalid section:\")\n",
    "                    print(\"standard_point: {}\".format(standard_point))\n",
    "                    norm_stand_point = [0.0, 0.0]\n",
    "            \n",
    "                cv.circle(standard_frame, standard_point, 1, (255,255, 0), 1)\n",
    "                custom_keypoints = np.append(custom_keypoints, norm_standard_point)\n",
    "            \n",
    "        else:\n",
    "            custom_keypoints = np.zeros(1086, dtype=np.float64)\n",
    "        current_frame += 1  \n",
    "        \n",
    "        if can_record:\n",
    "            sequence.append(custom_keypoints)\n",
    "            sequence = sequence[-35:]\n",
    "            \n",
    "        if len(sequence) == 35:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]      \n",
    "            predictions.append(np.argmax(res))\n",
    "\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):   \n",
    "                if res[np.argmax(res)] > threshold:                       \n",
    "                    predicted_text = actions[np.argmax(res)]\n",
    "                    can_do_live_text = True\n",
    "                    live_text_frame_number = 0\n",
    "                    alpha = 1.0                   \n",
    "                else:\n",
    "                    predicted_text = \"Low Accuracy\"\n",
    "                \n",
    "        if can_do_live_text:\n",
    "            if live_text_frame_number < 20:\n",
    "                live_text_frame_number += 1\n",
    "                alpha -= 0.5\n",
    "            else:\n",
    "                can_do_live_text = False\n",
    "                \n",
    "               \n",
    "        if np.all(idle_gest_reg == None):        \n",
    "            mode = \"idle\"\n",
    "        else:\n",
    "            mode = \"gest\"  \n",
    "            \n",
    "     \n",
    "        cv.putText(ori_img, mode, (3, 60), cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 0, cv.LINE_AA)\n",
    "        cv.putText(standard_frame, \"{}FN:{}\".format(mode, frame_number), (int(s_height/2), 20), cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 0, cv.LINE_AA)     \n",
    "        overlay = standard_frame.copy()\n",
    "        \n",
    "        if can_do_live_text:\n",
    "            cv.putText(overlay, predicted_text,(125, 280), cv.FONT_HERSHEY_SIMPLEX, 0.9, (255, 120, 50), 2)\n",
    "            cv.addWeighted(overlay, alpha, standard_frame, 1 - alpha, 0, standard_frame)\n",
    "        \n",
    "        cv.imshow('standard_Frame', standard_frame)            \n",
    "        cv.imshow('full_screen', ori_img)\n",
    "\n",
    "        if (cv.waitKey(1) & 0xFF == ord('q')):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
